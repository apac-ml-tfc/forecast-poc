{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Evaluating Predictors: Part 2 - Related Time Series\n",
    "\n",
    "This notebook will build off of all the earlier work and requires that at least the importing of target time series and related time series data be complete. If you have not performed those steps yet, go back, do so, then continue.\n",
    "\n",
    "At this point you now have a target-time-series dataset and a related-time-series dataset loaded into a singular Dataset Group, this is what is required to leverage the models that support related data in Amazon Forecast. If your data supports item level metadata it could be added to the dataset group as well and would benefit only algorithms that support that (e.g. CNN-QR, DeepAR+, but **not** Prophet). \n",
    "\n",
    "To continue the work, start with the imports, determine your region, establish your API connections, and load all previously stored values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "from pprint import pprint as prettyprint\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region)\n",
    "\n",
    "forecast = session.client(\"forecast\")\n",
    "forecast_query = session.client(\"forecastquery\")\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "export_bucket = s3.Bucket(export_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Predictors\n",
    " \n",
    "Given that that our data is hourly and we want to generate a forecast on the hour, Forecast limits us to a horizon of 500 of whatever the slice is. This means we will be able to predict about 20 days into the future.\n",
    "\n",
    "The cells below will define a few variables to be used with all of our models. We'll then re-use these to create each `Predictor` we investigate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 240\n",
    "num_backtest_windows = 1\n",
    "backtest_window_offset = 240\n",
    "forecast_frequency = \"H\"\n",
    "evaluation_parameters = {\n",
    "    \"NumberOfBacktestWindows\": 1,\n",
    "    \"BackTestWindowOffset\": 240,\n",
    "}\n",
    "input_data_config = {\n",
    "    \"DatasetGroupArn\": datasetGroupArn,\n",
    "    \"SupplementaryFeatures\": [\n",
    "        { \"Name\": \"holiday\", \"Value\": \"US\" },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_algorithm_arn = \"arn:aws:forecast:::algorithm/Prophet\"\n",
    "deeparp_algorithm_arn = \"arn:aws:forecast:::algorithm/Deep_AR_Plus\"\n",
    "cnnqr_algorithm_arn = \"arn:aws:forecast:::algorithm/CNN-QR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_prophet_rel_algo_1\",\n",
    "    AlgorithmArn=prophet_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": forecast_frequency,\n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "results[\"Prophet with RTS\"] = SimpleNamespace(predictor_arn=prophet_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeparp_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_deeparp_rel_algo_1\",\n",
    "    AlgorithmArn=deeparp_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": forecast_frequency,\n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "results[\"DeepAR+ with RTS\"] = SimpleNamespace(predictor_arn=deeparp_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnqr_create_predictor_response = forecast.create_predictor(\n",
    "#     PredictorName=f\"{project}_cnnqr_rel_algo_1\",\n",
    "#     AlgorithmArn=cnnqr_algorithm_arn,\n",
    "#     ForecastHorizon=forecast_horizon,\n",
    "#     PerformAutoML=False,\n",
    "#     PerformHPO=False,\n",
    "#     EvaluationParameters=evaluation_parameters,\n",
    "#     InputDataConfig=input_data_config,\n",
    "#     FeaturizationConfig={\n",
    "#         \"ForecastFrequency\": forecast_frequency,\n",
    "#         \"Featurizations\": [\n",
    "#             {\n",
    "#                 \"AttributeName\": \"target_value\",\n",
    "#                 \"FeaturizationPipeline\": [\n",
    "#                     {\n",
    "#                         \"FeaturizationMethodName\": \"filling\",\n",
    "#                         \"FeaturizationMethodParameters\": {\n",
    "#                             \"frontfill\": \"none\",\n",
    "#                             \"middlefill\": \"zero\",\n",
    "#                             \"backfill\": \"zero\",\n",
    "#                         },\n",
    "#                     },\n",
    "#                 ],\n",
    "#             },\n",
    "#         ],\n",
    "#     },\n",
    "# )\n",
    "# results[\"CNN-QR with RTS\"] = SimpleNamespace(predictor_arn=cnnqr_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally in our notebooks we would have a while loop that polls for each of these to determine the status of the models in training. For simplicity sake here we are going to rely on you opening a new browser tab and following along in the console until a predictor has been created for each algorithm. \n",
    "\n",
    "Your previous tab from opening this session of Jupyter Lab should still be open, from there navigate to the Amazon Forecast service page, then select your dataset group. Lastly click `Predictors` and you should see the creation in progress. Once they are active you are ready to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_predictors = [results[r].predictor_arn for r in results]\n",
    "failed_predictors = []\n",
    "\n",
    "def check_status():\n",
    "    \"\"\"Check and update in_progress_predictors\"\"\"\n",
    "    just_stopped = []  # Can't edit the in_progress list directly the loop!\n",
    "    for arn in in_progress_predictors:\n",
    "        predictor_desc = forecast.describe_predictor(PredictorArn=arn)\n",
    "        status = predictor_desc[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f\"\\nBuild succeeded for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "        elif \"FAILED\" in status:\n",
    "            print(f\"\\nBuild failed for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "            failed_predictors.append(arn)\n",
    "    for arn in just_stopped:\n",
    "        in_progress_predictors.remove(arn)\n",
    "    return in_progress_predictors\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=check_status,\n",
    "    fn_is_finished=lambda l: len(l) == 0,\n",
    "    fn_stringify_result=lambda l: f\"{len(l)} predictor builds in progress\",\n",
    "    poll_secs=60,  # Poll every minute\n",
    "    timeout_secs=3*60*60,  # Max 3 hours\n",
    ")\n",
    "\n",
    "if len(failed_predictors):\n",
    "    raise RuntimeError(f\"The following predictors failed to train:\\n{failed_predictors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Predictors\n",
    "\n",
    "Once each of the Predictors is in an `Active` state you can get metrics about it to better understand its accuracy and behavior. These are computed based on the hold out periods we defined when building the Predictor. The metrics are meant to guide our decisions when we use a particular Predictor to generate a forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trial_metrics(trial_name=None) -> pd.DataFrame:\n",
    "    \"\"\"Utility to fetch the accuracy metrics for a predictor and output the leaderboard so far\"\"\"\n",
    "    if (trial_name):\n",
    "        # Print the raw API response:\n",
    "        metrics_response = forecast.get_accuracy_metrics(PredictorArn=results[trial_name].predictor_arn)\n",
    "        print(f\"Raw metrics for {trial_name}:\")\n",
    "        prettyprint(metrics_response)\n",
    "\n",
    "        # Save the payload section to results:\n",
    "        evaluation_results = metrics_response[\"PredictorEvaluationResults\"]\n",
    "        results[trial_name].evaluation_results = evaluation_results\n",
    "\n",
    "        # Construct simplified version for our comparison:\n",
    "        try:\n",
    "            summary_metrics = next(\n",
    "                w for w in evaluation_results[0][\"TestWindows\"] if w[\"EvaluationType\"] == \"SUMMARY\"\n",
    "            )[\"Metrics\"]\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Couldn't find SUMMARY metrics in Forecast API response\")\n",
    "        results[trial_name].summary_metrics = {\n",
    "            \"RMSE\": summary_metrics[\"RMSE\"],\n",
    "            \"10% wQL\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.1\n",
    "            ),\n",
    "            \"50% wQL (MAPE)\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.5\n",
    "            ),\n",
    "            \"90% wQL\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.9\n",
    "            ),\n",
    "        }\n",
    "    # Render the leaderboard:\n",
    "    return pd.DataFrame([\n",
    "        { \"Predictor\": name, **results[name].summary_metrics } for name in results\n",
    "        if \"summary_metrics\" in results[name].__dict__\n",
    "    ]).set_index(\"Predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "\n",
    "Here we are going to look to see the metrics from this Predictor like the earlier sessions, we will now add the related data metrics to the table from the previous notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"Prophet with RTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+\n",
    "\n",
    "Same as Prophet, now you should look at the metrics from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"DeepAR+ with RTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_trial_metrics(\"CNN-QR with RTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
