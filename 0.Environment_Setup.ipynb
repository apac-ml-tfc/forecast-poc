{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "Amazon Forecast will access source data from (and export forecasts to) Amazon S3... So before we start using Forecast, we should set up our bucket(s) and permissions.\n",
    "\n",
    "Production environments will typically automate this setup via tools like [AWS CloudFormation](https://aws.amazon.com/cloudformation/) and the [AWS Cloud Development Kit](https://aws.amazon.com/cdk/).\n",
    "\n",
    "Since we're just experimenting, we'll instead use this notebook to keep the setup easily customizable for your environment. (Assuming you're running the notebook with appropriate IAM and S3 administrative permissions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import json\n",
    "import secrets\n",
    "import string\n",
    "from time import sleep\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # (AWS Python SDK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to an AWS Region\n",
    "\n",
    "Assuming you're running this notebook on [Amazon SageMaker](https://aws.amazon.com/sagemaker/), it will already be associated with a particular [AWS Region](https://aws.amazon.com/about-aws/global-infrastructure/) and be running with certain [AWS IAM Permissions](https://aws.amazon.com/iam/) (defined by the **notebook execution role**).\n",
    "\n",
    "If you're running the notebook locally, you may need to explicitly log in e.g. using `aws configure` from the [AWS CLI](https://aws.amazon.com/cli/), and set the specific region you'd like to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=None)  # To set a specific region, replace None with e.g. \"us-east-1\"\n",
    "region = session.region_name  # We'll save the configured region to initialize later notebooks\n",
    "print(region)\n",
    "%store region\n",
    "\n",
    "iam = session.client(\"iam\")\n",
    "s3 = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Bucket(s)\n",
    "\n",
    "Amazon Forecast will read historical data from S3, and may export forecasts to S3.\n",
    "\n",
    "By default, we'll create a single bucket for both with a partially-randomized name (since S3 bucket names must be globally unique).\n",
    "\n",
    "You can customize this setup (e.g. to use an existing bucket instead) and/or configure through the [Amazon S3 Console](https://s3.console.aws.amazon.com/s3/home).\n",
    "\n",
    "Just be sure to `%store` a valid `bucket_name` and `export_bucket_name` which exist in the same `region`: We'll use this below and in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a source data bucket name:\n",
    "bucket_name = \"{}-forecastpoc-{}\".format(\n",
    "    # AWS Account ID:\n",
    "    session.client(\"sts\").get_caller_identity().get(\"Account\"),\n",
    "    # Random string:\n",
    "    ''.join(secrets.choice(string.ascii_lowercase + string.digits) for i in range(8))\n",
    ")\n",
    "print(bucket_name)\n",
    "%store bucket_name\n",
    "\n",
    "# Create the bucket (assuming it's new):\n",
    "if region != \"us-east-1\":\n",
    "    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ \"LocationConstraint\": region })\n",
    "else:\n",
    "    s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll assume forecast exports can go in the same bucket:\n",
    "\n",
    "export_bucket_name = bucket_name\n",
    "%store export_bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAM Role for Forecast\n",
    "\n",
    "To access data in these buckets, Amazon Forecast needs permissions. This means creating a **role** with appropriate access the buckets and which can be assumed by the Forecast service.\n",
    "\n",
    "By default, we'll create a new role and attach necessary permissions here.\n",
    "\n",
    "You can customize this setup and/or configure through the [AWS IAM Console](https://console.aws.amazon.com/iam/home).\n",
    "\n",
    "Just be sure to `%store` a valid `forecast_role_arn`: We'll use this in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_role_name = \"ForecastRolePOC\"\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName=forecast_role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps({\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"forecast.amazonaws.com\",\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "            },\n",
    "        ]\n",
    "    }),\n",
    ")\n",
    "\n",
    "forecast_role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(forecast_role_arn)\n",
    "%store forecast_role_arn\n",
    "\n",
    "# Note that AmazonForecastFullAccess provides access to some specifically-named default S3 buckets as well,\n",
    "# but we just want it for the Forecast permissions themselves:\n",
    "iam.attach_role_policy(\n",
    "    RoleName=forecast_role_name,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonForecastFullAccess\",\n",
    ")\n",
    "\n",
    "# By default (since we're experimenting), this code attaches over-generous S3 permissions (full access):\n",
    "iam.attach_role_policy(\n",
    "    RoleName=forecast_role_name,\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3FullAccess\",\n",
    ")\n",
    "# You could instead use something like the below to give access to *only* the relevant buckets:\n",
    "# inline_s3_policy = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": \"s3:*\",\n",
    "#             \"Resource\": [\n",
    "#                 # (Assuming you're not running in a different partition e.g. aws-cn)\n",
    "#                 f\"arn:aws:s3:::{bucket_name}\",\n",
    "#                 f\"arn:aws:s3:::{bucket_name}/*\",\n",
    "#             ]\n",
    "#         },\n",
    "#     ],\n",
    "# }\n",
    "# if bucket_name != export_bucket_name:\n",
    "#     inline_s3_policy[\"Statement\"][0][\"Resource\"].append(f\"arn:aws:s3:::{export_bucket_name}\")\n",
    "#     inline_s3_policy[\"Statement\"][0][\"Resource\"].append(f\"arn:aws:s3:::{export_bucket_name}/*\")\n",
    "\n",
    "# iam.put_role_policy(\n",
    "#     RoleName=role_name,\n",
    "#     PolicyName=\"ForecastPoCBucketAccess\",\n",
    "#     PolicyDocument=json.dumps(inline_s3_policy)\n",
    "# )\n",
    "\n",
    "# IAM policy attachments *may* take up to a minute to propagate, so just to be safe:\n",
    "sleep(60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
