{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Evaluating Predictors: Part 1 - Target Time Series\n",
    "\n",
    "This notebook will build off of the earlier data processing that was performed in the validation sessions. If you have not completed that part yet, go back to [1.Validate_and_Import_Target_Time_Series_Data.ipynb](1.Validate_and_Import_Target_Time_Series_Data.ipynb) and complete it first before resuming.\n",
    "\n",
    "At this point you have target-time-series data loaded into Amazon Forecast inside a Dataset Group, this is what is required to use all of the models within Amazon Forecast. As an initial exploration we will evaluate the results from ARIMA, Prophet, and DeepAR+. We could have also included ETS or CNN-QR, but have left them out for time constraints. Similarly NPTS was left out as it specializes on spiky data or large gaps which our dataset does not have.\n",
    "\n",
    "The very first thing to do is start with our imports, establish a connection to the Forecast service, and then restore our variables from before. The cells below will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint as prettyprint\n",
    "import time\n",
    "from types import SimpleNamespace  # (because Python dict [\"key\"] notation can get boring)\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region)\n",
    "\n",
    "forecast = session.client(\"forecast\")\n",
    "forecast_query = session.client(\"forecastquery\")\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "export_bucket = s3.Bucket(export_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Predictors\n",
    " \n",
    "Given that that our data is hourly and we want to generate a forecast on the hour, Forecast limits us to a horizon of 500 of whatever the slice is. This means we will be able to predict about 20 days into the future.\n",
    "\n",
    "The cells below will define a few variables to be used with all of our models. We'll then re-use these to create each `Predictor` we investigate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline/statistical methods:\n",
    "arima_algorithm_arn = \"arn:aws:forecast:::algorithm/ARIMA\"\n",
    "ets_algorithm_arn = \"arn:aws:forecast:::algorithm/ETS\"\n",
    "\n",
    "# Probabilistic methods:\n",
    "npts_algorithm_arn = \"arn:aws:forecast:::algorithm/NPTS\"\n",
    "prophet_algorithm_arn = \"arn:aws:forecast:::algorithm/Prophet\"\n",
    "\n",
    "# Deep learning methods:\n",
    "deeparp_algorithm_arn = \"arn:aws:forecast:::algorithm/Deep_AR_Plus\"\n",
    "cnnqr_algorithm_arn = \"arn:aws:forecast:::algorithm/CNN-QR\"\n",
    "\n",
    "# Set up our top-level results dict:\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 240\n",
    "forecast_frequency = \"H\"\n",
    "evaluation_parameters = {\n",
    "    \"NumberOfBacktestWindows\": 1,\n",
    "    \"BackTestWindowOffset\": 240,\n",
    "}\n",
    "input_data_config = {\n",
    "    \"DatasetGroupArn\": datasetGroupArn,\n",
    "    \"SupplementaryFeatures\": [\n",
    "        { \"Name\": \"holiday\", \"Value\": \"US\" },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_arima_algo_1\",\n",
    "    AlgorithmArn=arima_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": forecast_frequency,\n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "results[\"ARIMA\"] = SimpleNamespace(predictor_arn=arima_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_prophet_algo_1\",\n",
    "    AlgorithmArn=prophet_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": forecast_frequency, \n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "results[\"Prophet\"] = SimpleNamespace(predictor_arn=prophet_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeparp_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=f\"{project}_deeparp_algo_1\",\n",
    "    AlgorithmArn=deeparp_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters=evaluation_parameters,\n",
    "    InputDataConfig=input_data_config,\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": forecast_frequency, \n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "results[\"DeepAR+\"] = SimpleNamespace(predictor_arn=deeparp_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR\n",
    "\n",
    "We've commented out CNN-QR sections because in our tests the algorithm takes significantly longer to train on the small sample dataset than DeepAR+ - with comparable accuracy. On many larger \"real\" datasets CNN-QR can be much faster, so we'd recommend experimenting with it on your own data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnnqr_create_predictor_response = forecast.create_predictor(\n",
    "#     PredictorName=f\"{project}_cnnqr_algo_1\",\n",
    "#     AlgorithmArn=cnnqr_algorithm_arn,\n",
    "#     ForecastHorizon=forecast_horizon,\n",
    "#     PerformAutoML=False,\n",
    "#     PerformHPO=False,\n",
    "#     EvaluationParameters=evaluation_parameters,\n",
    "#     InputDataConfig=input_data_config,\n",
    "#     FeaturizationConfig={\n",
    "#         \"ForecastFrequency\": forecast_frequency, \n",
    "#         \"Featurizations\": [\n",
    "#             {\n",
    "#                 \"AttributeName\": \"target_value\",\n",
    "#                 \"FeaturizationPipeline\": [\n",
    "#                     {\n",
    "#                         \"FeaturizationMethodName\": \"filling\",\n",
    "#                         \"FeaturizationMethodParameters\": {\n",
    "#                             \"frontfill\": \"none\",\n",
    "#                             \"middlefill\": \"zero\",\n",
    "#                             \"backfill\": \"zero\",\n",
    "#                         },\n",
    "#                     },\n",
    "#                 ],\n",
    "#             },\n",
    "#         ],\n",
    "#     },\n",
    "# )\n",
    "# results[\"CNN-QR\"] = SimpleNamespace(predictor_arn=cnnqr_create_predictor_response[\"PredictorArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally in our notebooks we would have a while loop that polls for each of these to determine the status of the models in training. The cell below will poll for the ARNs of each and return when they are all available so you can move onto the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_predictors = [results[r].predictor_arn for r in results]\n",
    "failed_predictors = []\n",
    "\n",
    "def check_status():\n",
    "    \"\"\"Check and update in_progress_predictors\"\"\"\n",
    "    just_stopped = []  # Can't edit the in_progress list directly the loop!\n",
    "    for arn in in_progress_predictors:\n",
    "        predictor_desc = forecast.describe_predictor(PredictorArn=arn)\n",
    "        status = predictor_desc[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f\"\\nBuild succeeded for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "        elif \"FAILED\" in status:\n",
    "            print(f\"\\nBuild failed for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "            failed_predictors.append(arn)\n",
    "    for arn in just_stopped:\n",
    "        in_progress_predictors.remove(arn)\n",
    "    return in_progress_predictors\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=check_status,\n",
    "    fn_is_finished=lambda l: len(l) == 0,\n",
    "    fn_stringify_result=lambda l: f\"{len(l)} predictor builds in progress\",\n",
    "    poll_secs=60,  # Poll every minute\n",
    "    timeout_secs=3*60*60,  # Max 3 hours\n",
    ")\n",
    "\n",
    "if len(failed_predictors):\n",
    "    raise RuntimeError(f\"The following predictors failed to train:\\n{failed_predictors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Predictors\n",
    "\n",
    "Once each of the Predictors is in an `Active` state you can get metrics about it to better understand its accuracy and behavior. These are computed based on the hold out periods we defined when building the Predictor. The metrics are meant to guide our decisions when we use a particular Predictor to generate a forecast.\n",
    "\n",
    "Below we'll define a utility function below which retrieves (and prints) the raw accuracy metrics response, and also builds up a leaderboard. In the following cells, we'll run the function against each trained predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trial_metrics(trial_name=None) -> pd.DataFrame:\n",
    "    \"\"\"Utility to fetch the accuracy metrics for a predictor and output the leaderboard so far\"\"\"\n",
    "    if (trial_name):\n",
    "        # Print the raw API response:\n",
    "        metrics_response = forecast.get_accuracy_metrics(PredictorArn=results[trial_name].predictor_arn)\n",
    "        print(f\"Raw metrics for {trial_name}:\")\n",
    "        prettyprint(metrics_response)\n",
    "\n",
    "        # Save the payload section to results:\n",
    "        evaluation_results = metrics_response[\"PredictorEvaluationResults\"]\n",
    "        results[trial_name].evaluation_results = evaluation_results\n",
    "\n",
    "        # Construct simplified version for our comparison:\n",
    "        try:\n",
    "            summary_metrics = next(\n",
    "                w for w in evaluation_results[0][\"TestWindows\"] if w[\"EvaluationType\"] == \"SUMMARY\"\n",
    "            )[\"Metrics\"]\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Couldn't find SUMMARY metrics in Forecast API response\")\n",
    "        results[trial_name].summary_metrics = {\n",
    "            \"RMSE\": summary_metrics[\"RMSE\"],\n",
    "            \"10% wQL\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.1\n",
    "            ),\n",
    "            \"50% wQL (MAPE)\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.5\n",
    "            ),\n",
    "            \"90% wQL\": next(\n",
    "                l[\"LossValue\"] for l in summary_metrics[\"WeightedQuantileLosses\"] if l[\"Quantile\"] == 0.9\n",
    "            ),\n",
    "        }\n",
    "    # Render the leaderboard:\n",
    "    return pd.DataFrame([\n",
    "        { \"Predictor\": name, **results[name].summary_metrics } for name in results\n",
    "        if \"summary_metrics\" in results[name].__dict__\n",
    "    ]).set_index(\"Predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "\n",
    "ARIMA is one of the gold standards for time series forecasting. This algorithm is not particularly sophisticated but it is reliable and can help us understand a baseline of performance. To note it does not really understand seasonality very well and it does not support any item metadata or related time series information. Due to that we will explore it here but not after adding other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"ARIMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test, ARIMA scored a RMSE of ~1950 and 50% weighted quantile loss (=MAPE) of ~0.4715. ARIMA results will help us benchmark other predictors, looking for a reduction versus these baseline loss figures. Your results may vary a little across the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "\n",
    "As with ARIMA, let's explore the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"Prophet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test, Prophet achieved a slightly lower RMSE than ARIMA at around 1910 - but the weighted quantile losses were pretty similar and worse on some quantiles.\n",
    "\n",
    "At this point, Prophet has not had a chance to use any of its abilities to integrate related time-series information since only the target time-series has been uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+\n",
    "\n",
    "As with Prophet and ARIMA, let's explore the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trial_metrics(\"DeepAR+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test, DeepAR+ achieved a significant improvement in accuracy as measured both by RMSE (~1570) and at the 50% and 90% quantiles. The 10% quantile showed somewhat poorer performance, but overall accuracy was still good.\n",
    "\n",
    "To see what all this looks like in a visual format, we'll now create a Forecast with each Predictor and then export them to S3 to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_trial_metrics(\"CNN-QR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Exporting Forecasts\n",
    "\n",
    "Inside Amazon Forecast a Forecast is a rendered collection of all of your items, at every time interval, for all selected quantiles, for your given forecast horizon. This process takes the Predictor you just created and uses it to generate these inferences and to store them in a useful state. Once a Forecast exists within the service you can query it and obtain a JSON response or use another API call to export it to a CSV that is stored in S3. \n",
    "\n",
    "This tutorial will focus on the S3 Export as that is often an easy way to manually explore the data with many tools.\n",
    "\n",
    "These again will take some time to complete after you have executed the cells so explore the console to see when they have completed.\n",
    "\n",
    "To do that visit the Amazon Forecast Service page, then clck your Dataset Group, and then click `Forecasts` on the left. They will say `Create in progress...` initially and then `Active` when ready for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=f\"{project}_arima_algo_forecast\",\n",
    "    PredictorArn=results[\"ARIMA\"].predictor_arn,\n",
    ")\n",
    "results[\"ARIMA\"].forecast_arn = create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=f\"{project}_prophet_algo_forecast\",\n",
    "    PredictorArn=results[\"Prophet\"].predictor_arn,\n",
    ")\n",
    "results[\"Prophet\"].forecast_arn = create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=f\"{project}_deeparp_algo_forecast\",\n",
    "    PredictorArn=results[\"DeepAR+\"].predictor_arn,\n",
    ")\n",
    "results[\"DeepAR+\"].forecast_arn = create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_forecast_response = forecast.create_forecast(\n",
    "#     ForecastName=f\"{project}_cnnqr_algo_forecast\",\n",
    "#     PredictorArn=results[\"CNN-QR\"].predictor_arn,\n",
    "# )\n",
    "# results[\"CNN-QR\"].forecast_arn = create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again as you saw in the training step, you should poll until completion to know that you are ready to proceed to the next step. The cell below will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_forecasts = [results[r].forecast_arn for r in results]\n",
    "failed_forecasts = []\n",
    "\n",
    "def check_status():\n",
    "    \"\"\"Check and update in_progress_forecasts\"\"\"\n",
    "    just_stopped = []  # Can't edit the in_progress list directly the loop!\n",
    "    for arn in in_progress_forecasts:\n",
    "        desc_response = forecast.describe_forecast(ForecastArn=arn)\n",
    "        status = desc_response[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f\"\\nBuild succeeded for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "        elif \"FAILED\" in status:\n",
    "            print(f\"\\nBuild failed for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "            failed_forecasts.append(arn)\n",
    "    for arn in just_stopped:\n",
    "        in_progress_forecasts.remove(arn)\n",
    "    return in_progress_forecasts\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=check_status,\n",
    "    fn_is_finished=lambda l: len(l) == 0,\n",
    "    fn_stringify_result=lambda l: f\"{len(l)} forecast builds in progress\",\n",
    "    poll_secs=60,  # Poll every 60s\n",
    "    timeout_secs=2*60*60,  # Max 2 hours\n",
    ")\n",
    "\n",
    "if len(failed_forecasts):\n",
    "    raise RuntimeError(f\"The following forecasts failed:\\n{failed_forecasts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once they are `Active` you can start the export process. The code to do so is in the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=\"arima_export\",\n",
    "    ForecastArn=results[\"ARIMA\"].forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": f\"s3://{export_bucket_name}/arima/01_TTS_Only/\",\n",
    "            \"RoleArn\": forecast_role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "results[\"ARIMA\"].export_arn = export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=\"prophet_export\",\n",
    "    ForecastArn=results[\"Prophet\"].forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": f\"s3://{export_bucket_name}/prophet/01_TTS_Only/\",\n",
    "            \"RoleArn\": forecast_role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "results[\"Prophet\"].export_arn = export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=\"deeparp_export\",\n",
    "    ForecastArn=results[\"DeepAR+\"].forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": f\"s3://{export_bucket_name}/deeparp/01_TTS_Only/\",\n",
    "            \"RoleArn\": forecast_role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "results[\"DeepAR+\"].export_arn = export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_response = forecast.create_forecast_export_job(\n",
    "#     ForecastExportJobName=\"cnnqr_export\",\n",
    "#     ForecastArn=results[\"CNN-QR\"].forecast_arn,\n",
    "#     Destination={\n",
    "#         \"S3Config\": {\n",
    "#             \"Path\": f\"s3://{export_bucket_name}/cnnqr/01_TTS_Only/\",\n",
    "#             \"RoleArn\": forecast_role_arn,\n",
    "#         },\n",
    "#     },\n",
    "# )\n",
    "# results[\"CNN-QR\"].export_arn = export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exporting process is another one of those items that will take several minutes to complete. Once again, poll with the cell below then move on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_exports = [results[r].export_arn for r in results]\n",
    "failed_exports = []\n",
    "\n",
    "def check_status():\n",
    "    \"\"\"Check and update in_progress_exports\"\"\"\n",
    "    just_stopped = []  # Can't edit the in_progress list directly the loop!\n",
    "    for arn in in_progress_exports:\n",
    "        desc_response = forecast.describe_forecast_export_job(ForecastExportJobArn=arn)\n",
    "        status = desc_response[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f\"\\nExport succeeded for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "        elif \"FAILED\" in status:\n",
    "            print(f\"\\nExport failed for {arn}\")\n",
    "            just_stopped.append(arn)\n",
    "            failed_exports.append(arn)\n",
    "    for arn in just_stopped:\n",
    "        in_progress_exports.remove(arn)\n",
    "    return in_progress_exports\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    fn_poll_result=check_status,\n",
    "    fn_is_finished=lambda l: len(l) == 0,\n",
    "    fn_stringify_result=lambda l: f\"{len(l)} forecast exports in progress\",\n",
    "    poll_secs=20,  # Poll every 20s\n",
    "    timeout_secs=60*60,  # Max 1 hour\n",
    ")\n",
    "\n",
    "if len(failed_exports):\n",
    "    raise RuntimeError(f\"The following exports failed:\\n{failed_exports}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Exports\n",
    "\n",
    "Once our forecasts are exported to Amazon S3, we can download them locally to the notebook for analysis and plotting.\n",
    "\n",
    "Although [Boto3 S3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html) provides functions for listing and fetching files, we'll simplify the process a bit by calling the `aws s3 sync` command from the [AWS CLI](https://aws.amazon.com/cli/).\n",
    "\n",
    "Note that large exports may split output into multiple files, so here we store *lists* of filenames for each export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc = forecast.describe_forecast_export_job(ForecastExportJobArn=results[\"ARIMA\"].export_arn)\n",
    "s3uri = job_desc[\"Destination\"][\"S3Config\"][\"Path\"]\n",
    "local_folder = f\"data/exports/{job_desc['ForecastExportJobName']}\"\n",
    "\n",
    "!aws s3 sync $s3uri $local_folder\n",
    "\n",
    "results[\"ARIMA\"].local_filenames = sorted(list(map(\n",
    "    lambda file: f\"{local_folder}/{file}\",\n",
    "    filter(lambda file: file.endswith(\".csv\"), os.listdir(local_folder)),\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc = forecast.describe_forecast_export_job(ForecastExportJobArn=results[\"Prophet\"].export_arn)\n",
    "s3uri = job_desc[\"Destination\"][\"S3Config\"][\"Path\"]\n",
    "local_folder = f\"data/exports/{job_desc['ForecastExportJobName']}\"\n",
    "\n",
    "!aws s3 sync $s3uri $local_folder\n",
    "\n",
    "results[\"Prophet\"].local_filenames = sorted(list(map(\n",
    "    lambda file: f\"{local_folder}/{file}\",\n",
    "    filter(lambda file: file.endswith(\".csv\"), os.listdir(local_folder)),\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc = forecast.describe_forecast_export_job(ForecastExportJobArn=results[\"DeepAR+\"].export_arn)\n",
    "s3uri = job_desc[\"Destination\"][\"S3Config\"][\"Path\"]\n",
    "local_folder = f\"data/exports/{job_desc['ForecastExportJobName']}\"\n",
    "\n",
    "!aws s3 sync $s3uri $local_folder\n",
    "\n",
    "results[\"DeepAR+\"].local_filenames = sorted(list(map(\n",
    "    lambda file: f\"{local_folder}/{file}\",\n",
    "    filter(lambda file: file.endswith(\".csv\"), os.listdir(local_folder)),\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_desc = forecast.describe_forecast_export_job(ForecastExportJobArn=results[\"CNN-QR\"].export_arn)\n",
    "# s3uri = job_desc[\"Destination\"][\"S3Config\"][\"Path\"]\n",
    "# local_folder = f\"data/exports/{job_desc['ForecastExportJobName']}\"\n",
    "\n",
    "# !aws s3 sync $s3uri $local_folder\n",
    "\n",
    "# results[\"CNN-QR\"].local_filenames = sorted(list(map(\n",
    "#     lambda file: f\"{local_folder}/{file}\",\n",
    "#     filter(lambda file: file.endswith(\".csv\"), os.listdir(local_folder)),\n",
    "# )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Now it's time to explore the downloaded results to plot the forecasts against our validation data and check the format is as expected.\n",
    "\n",
    "> ⚠️ **Note:** Because our dataset is small, we'll just load a single (first) file from each export to simplify the code here... Be careful if re-using this code for bigger datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "\n",
    "We'll take things slowly with our first (ARIMA) forecast to demonstrate the details, and then speed things up for the others.\n",
    "\n",
    "First, let's take a look at the raw DataFrame as loaded by Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {results['ARIMA'].local_filenames[0]}...\")\n",
    "arima_predicts = pd.read_csv(results[\"ARIMA\"].local_filenames[0])\n",
    "arima_predicts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tidy things up a bit by parsing the timestamps and indexing the dataframe by them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to datetime\n",
    "arima_predicts[\"date\"] = pd.to_datetime(arima_predicts[\"date\"])\n",
    "arima_predicts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the timezone and make date the index\n",
    "arima_predicts['date'] = arima_predicts['date'].dt.tz_convert(None)\n",
    "arima_predicts.set_index('date', inplace=True)\n",
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arima_predicts.index.min())\n",
    "print(arima_predicts.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our prediction goes from Jan 01 to Jan 10 as expected given our 240 interval forecast horizon. Also we can see the cyclical nature of the predictions over the entire timeframe.\n",
    "\n",
    "To visualize our forecast against the validation data, it would be helpful to:\n",
    "\n",
    "- Remove/deal with the `item_id` column (which is always constant for our our single-item sample data)\n",
    "- Overlay the actual value from the validation data\n",
    "\n",
    "Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out item_id:\n",
    "arima_simple = arima_predicts[['p10', 'p50', 'p90']]\n",
    "arima_simple.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the time window of validation data we'd like to overlay:\n",
    "validation_df = validation_time_series_df.rename(columns={\"traffic_volume\": \"actual\"}).loc[\"2018-01-01\":\"2018-01-10\"]\n",
    "print(validation_df.index.min())\n",
    "print(validation_df.index.max())\n",
    "validation_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dataframes together:\n",
    "arima_val_df = arima_simple.join(validation_df, how=\"outer\")\n",
    "arima_val_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this particular plot is hard to see, let us pick a random day January 5th to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_val_df.loc[\"2018-01-05\":\"2018-01-06\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the actual traffic tracks quite closely to the p50 median prediction, and should stay within the p10-p90 band for most or all of the forecast horizon.\n",
    "\n",
    "As a final step, we've implemented a utility function to improve the clarity a little further by expanding the plot area and plotting the p10/p90 interval as a **confidence interval** rather than independent lines.\n",
    "\n",
    "Check you're comfortable with the plot below, and then we'll move on to comparing ARIMA with our other predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_forecasts(arima_predicts, actuals=validation_df, ylabel=\"Traffic Volume\")\n",
    "\n",
    "util.plot_forecasts(\n",
    "    arima_predicts[\"2018-01-05\":\"2018-01-06\"],\n",
    "    actuals=validation_df[\"2018-01-05\":\"2018-01-06\"],\n",
    "    ylabel=\"Traffic Volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {results['Prophet'].local_filenames[0]}...\")\n",
    "prophet_predicts = pd.read_csv(results[\"Prophet\"].local_filenames[0])\n",
    "\n",
    "# Set up date index:\n",
    "prophet_predicts[\"date\"] = pd.to_datetime(prophet_predicts[\"date\"]).dt.tz_convert(None)\n",
    "prophet_predicts.set_index(\"date\", inplace=True)\n",
    "\n",
    "util.plot_forecasts(prophet_predicts, actuals=validation_df, ylabel=\"Traffic Volume\")\n",
    "\n",
    "util.plot_forecasts(\n",
    "    prophet_predicts.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "    actuals=validation_df.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "    ylabel=\"Traffic Volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {results['DeepAR+'].local_filenames[0]}...\")\n",
    "deeparp_predicts = pd.read_csv(results[\"DeepAR+\"].local_filenames[0])\n",
    "\n",
    "# Set up date index:\n",
    "deeparp_predicts[\"date\"] = pd.to_datetime(deeparp_predicts[\"date\"]).dt.tz_convert(None)\n",
    "deeparp_predicts.set_index(\"date\", inplace=True)\n",
    "\n",
    "util.plot_forecasts(deeparp_predicts, actuals=validation_df, ylabel=\"Traffic Volume\")\n",
    "\n",
    "util.plot_forecasts(\n",
    "    deeparp_predicts.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "    actuals=validation_df.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "    ylabel=\"Traffic Volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is particularly interesting here is that even the p90 prediction is significantly below the actual numbers for a good portion of some days.\n",
    "\n",
    "Consider how the evaluation metrics of these algorithms relate to the observed performance characteristics in the validation plots. Clearly for probabilistic forecasts, central metrics like RMSE and MAPE/wQL0.5 tell only part of the story of model accuracy - and this is why additional quantile loss metrics are presented.\n",
    "\n",
    "Note that different algorithms generate quantiles by different methods. In particular, CNN-QR does not guarantee the ordering of quantiles (although good-quality fits should converge towards quantiles being ordered) - so there might be brief periods where e.g. the `p10` forecast quantile is higher than `p50`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Loading {results['CNN-QR'].local_filenames[0]}...\")\n",
    "# cnnqr_predicts = pd.read_csv(results[\"CNN-QR\"].local_filenames[0])\n",
    "\n",
    "# # Set up date index:\n",
    "# cnnqr_predicts[\"date\"] = pd.to_datetime(cnnqr_predicts[\"date\"]).dt.tz_convert(None)\n",
    "# cnnqr_predicts.set_index(\"date\", inplace=True)\n",
    "\n",
    "# util.plot_forecasts(cnnqr_predicts, actuals=validation_df, ylabel=\"Traffic Volume\")\n",
    "\n",
    "# util.plot_forecasts(\n",
    "#     cnnqr_predicts.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "#     actuals=validation_df.loc[\"2018-01-05\":\"2018-01-06\"],\n",
    "#     ylabel=\"Traffic Volume\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and Next Steps\n",
    "\n",
    "We've now explored some initial models on the target timeseries alone, and can start exploring additional **related data** as a way to improve forecast accuracy. The next notebook will guide you through the process of importing some *related time-series*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
