{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Evaluating Predictors: Part 1 - Target Time Series\n",
    "\n",
    "This notebook will build off of the earlier data processing that was performed in the validation sessions. If you have not completed that part yet, go back to [1.Validate_and_Import_Target_Time_Series_Data.ipynb](1.Validate_and_Import_Target_Time_Series_Data.ipynb) and complete it first before resuming.\n",
    "\n",
    "At this point you have target-time-series data loaded into Amazon Forecast inside a Dataset Group, this is what is required to use all of the models within Amazon Forecast. As an initial exploration we will evaluate the results from ARIMA, Prophet, and CNN-QR. We could have also included ETS or DeepAR+, but have left them out for time constraints. Similarly NPTS was left out as it specializes on spiky data or large gaps which our dataset does not have.\n",
    "\n",
    "The very first thing to do is start with our imports, establish a connection to the Forecast service, and then restore our variables from before. The cells below will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import json\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=region)\n",
    "\n",
    "forecast = session.client(\"forecast\")\n",
    "forecast_query = session.client(\"forecastquery\")\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "poc_bucket = s3.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Predictors\n",
    " \n",
    "Given that that our data is hourly and we want to generate a forecast on the hour, Forecast limits us to a horizon of 500 of whatever the slice is. This means we will be able to predict about 20 days into the future.\n",
    "\n",
    "The cells below will define a few variables to be used with all of our models. Then there will be an API call to create each `Predictor` where they are based on ARIMA, Prophet, and CNN-QR respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 240\n",
    "num_backtest_windows = 1\n",
    "backtest_window_offset = 240\n",
    "forecast_frequency = \"H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_algorithm_arn = \"arn:aws:forecast:::algorithm/ARIMA\"\n",
    "prophet_algorithm_arn = \"arn:aws:forecast:::algorithm/Prophet\"\n",
    "cnnqr_algorithm_arn = \"arn:aws:forecast:::algorithm/CNN-QR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Specifics\n",
    "arima_predictor_name = project + \"_arima_algo_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ARIMA:\n",
    "arima_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=arima_predictor_name,\n",
    "    AlgorithmArn=arima_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters={\n",
    "        \"NumberOfBacktestWindows\": num_backtest_windows,\n",
    "        \"BackTestWindowOffset\": backtest_window_offset,\n",
    "    },\n",
    "    InputDataConfig={\n",
    "        \"DatasetGroupArn\": datasetGroupArn,\n",
    "        \"SupplementaryFeatures\": [\n",
    "            { \"Name\": \"holiday\", \"Value\": \"US\" },\n",
    "        ],\n",
    "    },\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": forecast_frequency,\n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Specifics\n",
    "prophet_predictor_name = project + \"_prophet_algo_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Prophet:\n",
    "prophet_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=prophet_predictor_name,\n",
    "    AlgorithmArn=prophet_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters={\n",
    "        \"NumberOfBacktestWindows\": num_backtest_windows,\n",
    "        \"BackTestWindowOffset\": backtest_window_offset,\n",
    "    },\n",
    "    InputDataConfig={\n",
    "        \"DatasetGroupArn\": datasetGroupArn,\n",
    "        \"SupplementaryFeatures\": [\n",
    "            { \"Name\": \"holiday\", \"Value\": \"US\" },\n",
    "        ],\n",
    "    },\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": forecast_frequency, \n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-QR Specifics\n",
    "cnnqr_predictor_name = project + \"_cnnqr_algo_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN-QR:\n",
    "cnnqr_create_predictor_response = forecast.create_predictor(\n",
    "    PredictorName=cnnqr_predictor_name,\n",
    "    AlgorithmArn=cnnqr_algorithm_arn,\n",
    "    ForecastHorizon=forecast_horizon,\n",
    "    PerformAutoML=False,\n",
    "    PerformHPO=False,\n",
    "    EvaluationParameters={\n",
    "        \"NumberOfBacktestWindows\": num_backtest_windows,\n",
    "        \"BackTestWindowOffset\": backtest_window_offset,\n",
    "    },\n",
    "    InputDataConfig={\n",
    "        \"DatasetGroupArn\": datasetGroupArn,\n",
    "        \"SupplementaryFeatures\": [\n",
    "            { \"Name\": \"holiday\", \"Value\": \"US\" },\n",
    "        ],\n",
    "    },\n",
    "    FeaturizationConfig={\n",
    "        \"ForecastFrequency\": forecast_frequency, \n",
    "        \"Featurizations\": [\n",
    "            {\n",
    "                \"AttributeName\": \"target_value\",\n",
    "                \"FeaturizationPipeline\": [\n",
    "                    {\n",
    "                        \"FeaturizationMethodName\": \"filling\",\n",
    "                        \"FeaturizationMethodParameters\": {\n",
    "                            \"frontfill\": \"none\",\n",
    "                            \"middlefill\": \"zero\",\n",
    "                            \"backfill\": \"zero\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally in our notebooks we would have a while loop that polls for each of these to determine the status of the models in training. The cell below will poll for the ARNs of each and return when they are all available so you can move onto the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_predictors = [\n",
    "    arima_create_predictor_response[\"PredictorArn\"],\n",
    "    prophet_create_predictor_response[\"PredictorArn\"],\n",
    "    cnnqr_create_predictor_response[\"PredictorArn\"],\n",
    "]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for predictor_arn in in_progress_predictors:\n",
    "        predictor_response = forecast.describe_predictor(PredictorArn=predictor_arn)\n",
    "        status = predictor_response[\"Status\"]\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Build succeeded for {}\".format(predictor_arn))\n",
    "            in_progress_predictors.remove(predictor_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Build failed for {}\".format(predictor_arn))\n",
    "            in_progress_predictors.remove(predictor_arn)\n",
    "\n",
    "    n_in_progress = len(in_progress_predictors)\n",
    "    if n_in_progress <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"{} predictor builds still in progress\".format(n_in_progress))\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Predictors\n",
    "\n",
    "Once each of the Predictors is in an `Active` state you can get metrics about it to better understand its accuracy and behavior. These are computed based on the hold out periods we defined when building the Predictor. The metrics are meant to guide our decisions when we use a particular Predictor to generate a forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA\n",
    "\n",
    "ARIMA is one of the gold standards for time series forecasting. This algorithm is not particularly sophisticated but it is reliable and can help us understand a baseline of performance. To note it does not really understand seasonality very well and it does not support any item metadata or related time series information. Due to that we will explore it here but not after adding other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Metrics\n",
    "arima_arn = arima_create_predictor_response[\"PredictorArn\"]\n",
    "arima_metrics = forecast.get_accuracy_metrics(PredictorArn=arima_arn)\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(arima_metrics)\n",
    "arima_RMSEs = util.extract_json_values(arima_metrics, \"RMSE\")\n",
    "markdown_results = []\n",
    "markdown_results.append(arima_RMSEs[0])\n",
    "arima_loss_values = util.extract_json_values(arima_metrics, \"LossValue\")\n",
    "markdown_results = markdown_results + arima_loss_values[::-1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\"\"\"\n",
    "Overall our RMSE is {0[0]}... This will help you rank your other predictors where you look to see a \n",
    "reduction in RMSE. Overall performance looks like this:\n",
    "\n",
    "| Predictor | RMSE               | 10%                 | 50%                 | 90%                |\n",
    "|-----------|--------------------|---------------------|---------------------|--------------------|\n",
    "| ARIMA     | {0[0]}             | {0[1]}              | {0[2]}              | {0[3]}             |\n",
    "\n",
    "Again these particular values will help us evaluate the other predictors.\n",
    "\"\"\".format(markdown_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "\n",
    "Same as ARIMA, now you should look at the metrics from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Metrics\n",
    "prophet_arn = prophet_create_predictor_response[\"PredictorArn\"]\n",
    "prophet_metrics = forecast.get_accuracy_metrics(PredictorArn=prophet_arn)\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(prophet_metrics)\n",
    "prophet_RMSEs = util.extract_json_values(prophet_metrics, \"RMSE\")\n",
    "markdown_results.append(prophet_RMSEs[0])\n",
    "prophet_loss_values = util.extract_json_values(prophet_metrics, \"LossValue\")\n",
    "markdown_results = markdown_results + prophet_loss_values[::-1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\"\"\"\n",
    "Here you see an RMSE of {0[4]}.... Just a bit lesser than ARIMA, but at this point Prophet has \n",
    "not had a chance to use any of its abilities to integrate related time series information. \n",
    "Prophet's performance compared to ARIMA is:\n",
    "\n",
    "| Predictor | RMSE               | 10%                 | 50%                 | 90%                |\n",
    "|-----------|--------------------|---------------------|---------------------|--------------------|\n",
    "| ARIMA     | {0[0]}             | {0[1]}              | {0[2]}              | {0[3]}             |\n",
    "| Prophet   | {0[4]}             | {0[5]}              | {0[6]}              | {0[7]}             |\n",
    "\n",
    "What this tells us is that when querying the 90% quantile we see less of an error from Prophet, but we see a \n",
    "bit worse performance in the 10% and 50% quantile. Next will be DeepAR.\n",
    "\"\"\".format(markdown_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR\n",
    "\n",
    "Same as Prophet and ARIMA, now you should look at the metrics from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-QR Metrics\n",
    "cnnqr_arn = cnnqr_create_predictor_response[\"PredictorArn\"]\n",
    "cnnqr_metrics = forecast.get_accuracy_metrics(PredictorArn=cnnqr_arn)\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(cnnqr_metrics)\n",
    "cnnqr_RMSEs = util.extract_json_values(cnnqr_metrics, \"RMSE\")\n",
    "markdown_results.append(cnnqr_RMSEs[0])\n",
    "cnnqr_loss_values = util.extract_json_values(cnnqr_metrics, \"LossValue\")\n",
    "markdown_results = markdown_results + cnnqr_loss_values[::-1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\"\"\"\n",
    "Here you see an RMSE of {0[8]}... Lesser and better than both ARIMA and Prophet. Diving a bit more deeply we see:\n",
    "\n",
    "| Predictor | RMSE               | 10%                 | 50%                 | 90%                |\n",
    "|-----------|--------------------|---------------------|---------------------|--------------------|\n",
    "| ARIMA     | {0[0]}             | {0[1]}              | {0[2]}              | {0[3]}             |\n",
    "| Prophet   | {0[4]}             | {0[5]}              | {0[6]}              | {0[7]}             |\n",
    "| CNN-QR    | {0[8]}             | {0[9]}              | {0[10]}             | {0[11]}            |\n",
    "\n",
    "We are now seeing major improvements in accuracy for the 50% and 90% quantiles with a bit worse performance on the 10%. \n",
    "To explore what this all looks like in a visual format we will now create a Forecast with each Predictor and then \n",
    "export it to s3 where we can download and explore the results.\n",
    "\"\"\".format(markdown_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Exporting Forecasts\n",
    "\n",
    "Inside Amazon Forecast a Forecast is a rendered collection of all of your items, at every time interval, for all selected quantiles, for your given forecast horizon. This process takes the Predictor you just created and uses it to generate these inferences and to store them in a useful state. Once a Forecast exists within the service you can query it and obtain a JSON response or use another API call to export it to a CSV that is stored in S3. \n",
    "\n",
    "This tutorial will focus on the S3 Export as that is often an easy way to manually explore the data with many tools.\n",
    "\n",
    "These again will take some time to complete after you have executed the cells so explore the console to see when they have completed.\n",
    "\n",
    "To do that visit the Amazon Forecast Service page, then clck your Dataset Group, and then click `Forecasts` on the left. They will say `Create in progress...` initially and then `Active` when ready for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA\n",
    "arima_forecast_name = project + \"_arima_algo_forecast\"\n",
    "arima_create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=arima_forecast_name,\n",
    "    PredictorArn=arima_arn,\n",
    ")\n",
    "arima_forecast_arn = arima_create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "prophet_forecast_name = project + \"_prophet_algo_forecast\"\n",
    "prophet_create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=prophet_forecast_name,\n",
    "    PredictorArn=prophet_arn,\n",
    ")\n",
    "prophet_forecast_arn = prophet_create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-QR\n",
    "cnnqr_forecast_name = project + \"_cnnqr_algo_forecast\"\n",
    "cnnqr_create_forecast_response = forecast.create_forecast(\n",
    "    ForecastName=cnnqr_forecast_name,\n",
    "    PredictorArn=cnnqr_arn,\n",
    ")\n",
    "cnnqr_forecast_arn = cnnqr_create_forecast_response[\"ForecastArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again as you saw in the training step, you should poll until completion to know that you are ready to proceed to the next step. The cell below will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_forecasts = [\n",
    "    arima_forecast_arn,\n",
    "    prophet_forecast_arn,\n",
    "    cnnqr_forecast_arn,\n",
    "]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for forecast_arn in in_progress_forecasts:\n",
    "        forecast_response = forecast.describe_forecast(ForecastArn=forecast_arn)\n",
    "        status = forecast_response[\"Status\"]\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Build succeeded for {}\".format(forecast_arn))\n",
    "            in_progress_forecasts.remove(forecast_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Build failed for {}\".format(forecast_arn))\n",
    "            in_progress_forecasts.remove(forecast_arn)\n",
    "\n",
    "    n_in_progress = len(in_progress_forecasts)\n",
    "    if n_in_progress <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"{} forecast builds still in progress\".format(n_in_progress))\n",
    "\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once they are `Active` you can start the export process. The code to do so is in the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_path = \"s3://\" + bucket_name + \"/arima_1/\"\n",
    "arima_job_name = \"ARIMAExport\"\n",
    "arima_export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=arima_job_name,\n",
    "    ForecastArn=arima_forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": arima_path,\n",
    "            \"RoleArn\": role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "armia_export_arn = arima_export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_path = \"s3://\" + bucket_name + \"/prophet_1/\"\n",
    "prophet_job_name = \"ProphetExport\"\n",
    "prophet_export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=prophet_job_name,\n",
    "    ForecastArn=prophet_forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": prophet_path,\n",
    "            \"RoleArn\": role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "prophet_export_arn = prophet_export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnqr_path = \"s3://\" + bucket_name + \"/cnnqr_1/\"\n",
    "cnnqr_job_name = \"CNNQRExport\"\n",
    "cnnqr_export_response = forecast.create_forecast_export_job(\n",
    "    ForecastExportJobName=cnnqr_job_name,\n",
    "    ForecastArn=cnnqr_forecast_arn,\n",
    "    Destination={\n",
    "        \"S3Config\": {\n",
    "            \"Path\": cnnqr_path,\n",
    "            \"RoleArn\": role_arn,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "cnnqr_export_arn = cnnqr_export_response[\"ForecastExportJobArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exporting process is another one of those items that will take several minutes to complete. Once again, poll with the cell below then move on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_exports = [\n",
    "    armia_export_arn,\n",
    "    prophet_export_arn,\n",
    "    cnnqr_export_arn,\n",
    "]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for export_arn in in_progress_exports:\n",
    "        export_response = forecast.describe_forecast_export_job(ForecastExportJobArn=export_arn)\n",
    "        status = export_response[\"Status\"]\n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Export succeeded for {}\".format(export_arn))\n",
    "            in_progress_exports.remove(export_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Export failed for {}\".format(export_arn))\n",
    "            in_progress_exports.remove(export_arn)\n",
    "    \n",
    "    n_in_progress = len(in_progress_exports)\n",
    "    if n_in_progress <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"{} forecast exports still in progress\".format(n_in_progress))\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the Forecasts\n",
    "\n",
    "At this point they are all exported into S3 but you need to obtain the results locally so we can explore them, the cells below will do that starting with ARIMA, then Prophet, and lastly CNN-QR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA\n",
    "arima_filename = \"\"\n",
    "arima_files = list(poc_bucket.objects.filter(Prefix=\"arima_1\"))\n",
    "for file in arima_files:\n",
    "    # There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key:\n",
    "        arima_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, data_dir+\"/\"+arima_filename)\n",
    "        break\n",
    "print(arima_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "prophet_filename = \"\"\n",
    "prophet_files = list(poc_bucket.objects.filter(Prefix=\"prophet_1\"))\n",
    "for file in prophet_files:\n",
    "    # There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key:\n",
    "        prophet_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, data_dir+\"/\"+prophet_filename)\n",
    "        break\n",
    "print(prophet_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-QR\n",
    "cnnqr_filename = \"\"\n",
    "cnnqr_files = list(poc_bucket.objects.filter(Prefix=\"cnnqr_1\"))\n",
    "for file in cnnqr_files:\n",
    "    # There will be a collection of CSVs if the forecast is large, modify this to go get them all\n",
    "    if \"csv\" in file.key:\n",
    "        cnnqr_filename = file.key.split('/')[1]\n",
    "        s3.Bucket(bucket_name).download_file(file.key, data_dir+\"/\"+cnnqr_filename)\n",
    "        break\n",
    "print(cnnqr_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Eval\n",
    "arima_predicts = pd.read_csv(data_dir+\"/\"+arima_filename)\n",
    "arima_predicts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to datetime\n",
    "arima_predicts['date'] = pd.to_datetime(arima_predicts['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the timezone and make date the index\n",
    "arima_predicts['date'] = arima_predicts['date'].dt.tz_convert(None)\n",
    "arima_predicts.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (arima_predicts.index.min())\n",
    "print (arima_predicts.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our prediction goes from Jan 01 to Jan 10 as expected given our 240 interval forecast horizon. Also we can see the cyclical nature of the predictions over the entire timeframe. \n",
    "\n",
    "Now we are going to create a dataframe of the prediction values from this Forecast and the actual values.\n",
    "\n",
    "First let us remove the column ID of item before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicts = arima_predicts[['p10', 'p50', 'p90']]\n",
    "arima_predicts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets slice validation to meet our needs\n",
    "validation_df = validation_time_series_df.copy()\n",
    "validation_df = validation_df.loc['2018-01-01':'2018-01-10']\n",
    "print (validation_df.index.min())\n",
    "print (validation_df.index.max())\n",
    "validation_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let us join the dataframes together\n",
    "arima_val_df = arima_predicts.join(validation_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "arima_val_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this particular plot is hard to see, let us pick a random day January 5th to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_val_df_jan_5 = arima_val_df.loc['2018-01-05':'2018-01-06']\n",
    "arima_val_df_jan_5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is pretty clear for p50 showcasing that it does a great job of predicting the volume. Let us now do this for Prophet and DeepAR+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet Validation\n",
    "\n",
    "We will speed up the prep work to just a few cells this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Eval\n",
    "prophet_predicts = pd.read_csv(data_dir+\"/\"+prophet_filename)\n",
    "prophet_predicts.sample()\n",
    "# Remove the timezone\n",
    "prophet_predicts['date'] = pd.to_datetime(prophet_predicts['date'])\n",
    "prophet_predicts['date'] = prophet_predicts['date'].dt.tz_convert(None)\n",
    "prophet_predicts.set_index('date', inplace=True)\n",
    "prophet_predicts = prophet_predicts[['p10', 'p50', 'p90']]\n",
    "# Finally let us join the dataframes together\n",
    "prophet_val_df = prophet_predicts.join(validation_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "prophet_val_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_val_df_jan_5 = prophet_val_df.loc['2018-01-05':'2018-01-06']\n",
    "prophet_val_df_jan_5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-QR Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-QR Eval\n",
    "cnnqr_predicts = pd.read_csv(data_dir + \"/\" + cnnqr_filename)\n",
    "cnnqr_predicts.sample()\n",
    "# Remove the timezone\n",
    "cnnqr_predicts[\"date\"] = pd.to_datetime(cnnqr_predicts[\"date\"])\n",
    "cnnqr_predicts[\"date\"] = cnnqr_predicts[\"date\"].dt.tz_convert(None)\n",
    "cnnqr_predicts.set_index(\"date\", inplace=True)\n",
    "cnnqr_predicts = cnnqr_predicts[[\"p10\", \"p50\", \"p90\"]]\n",
    "# Finally let us join the dataframes together\n",
    "cnnqr_val_df = cnnqr_predicts.join(validation_df, how=\"outer\")\n",
    "cnnqr_val_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnqr_val_df_jan_5 = cnnqr_val_df.loc[\"2018-01-05\":\"2018-01-06\"]\n",
    "cnnqr_val_df_jan_5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is particularly interesting here is that we were below the actual numbers for a good portion of the day even with p90. We did see great performance from Prophet and the metrics indicate that CNN-QR is objectively better here so now we will add related time series data to our project and see how the models behave then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and Next Steps\n",
    "\n",
    "At this point we can now see through the 3 plots below that CNN-QR does a really good job outside of the high ranges, and that perhaps adding related data could improve both Prophet and CNN-QR's performance. The next thing to do is to move to the notebook for importing your related-time series data and then progress to the second Creating and Evaluating notebook that will explain how to leverage the related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store markdown_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
